# Lecture 07

**Date**: Nov 16, 2020

**Slides**: https://docs.google.com/presentation/d/1tzvwrHQPb_b1H5W1bhV6S3F4SvAgwki1iocxgqG238o/edit?usp=sharing

NLP

* Language model (LM)
* Recurrent NN (RNN)
* LSTM, GRU


## Examples (on the lesson)

* [Generating text from Nietzscheâ€™s writings with Tensorflow/R](https://keras.rstudio.com/articles/examples/lstm_text_generation.html)
* [Writing Hamilton Lyrics with Tensorflow/R](https://www.kaggle.com/anasofiauzsoy/writing-hamilton-lyrics-with-tensorflow-r)
* [Irish songs generator colab (NLP Zero To Hero 4-6)](https://goo.gle/3aSTLGx)


## Videos

* [NLP Zero To Hero 4](https://www.youtube.com/watch?v=fNxaJsNG3-s&list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S&index=4)
* [NLP Zero To Hero 5](https://www.youtube.com/watch?v=r9QjkdSJZ2g&list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S&index=5)
* [NLP Zero To Hero 6](https://www.youtube.com/watch?v=Y_hzMnRXjhI&list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S&index=6)


## Additional materials (to be added later)

* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* For LSTM, GRU more detailed and mathematical explanation, see either [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd edition)](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646) (even the 1st edition), or Andrew Ng videos on Coursera - [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models/home/welcome)
* [Julia Silge's text generation in R](https://juliasilge.com/blog/tensorflow-generation/): note that at the end she recommends Markov chain model with [MarkovifyR](https://github.com/abresler/markovifyR). 


## Asignment 07

Select a corpus of your choise and train LM (examples are on [the last slide](https://docs.google.com/presentation/d/1tzvwrHQPb_b1H5W1bhV6S3F4SvAgwki1iocxgqG238o/edit#slide=id.ga0cf3ba537_0_104)). Then generate a few examples of the text. Did you manage to make it believable (or at least funny)?